import ollama from "ollama";

export default async function handler(req, res) {
  switch (req.method) {
    case "POST":
      const { question, user_answer } = req.body;

  const systemPrompt = `
        you are experianced coding assinstant.
        your final output should be the evaluation of data after executing code.
        Provide evaluation only without any explanation.
        Tell user that answer is correct or not.
        Provide final output in following json format:
        {
          question: "",
          user_answer: "",
          evaluation_result: "",
          correct_answer: ""
        }
      `;
  const response = await ollama.chat({
    model: "qwen2.5-coder:14b",
    messages: [
      {
        role: "system",
        content: systemPrompt,
      },
      {
        role: "user",
        content: `
              Question: ${question}
              Answer: ${user_answer}
            `,
      },
    ],
    //stream: true
  });
  console.log(response.message.content);

  

  res.status(200).json({ response: "ollama response" });
      break;
    default:
      res.setHeader("Allow", ["GET", "POST"]);
      res.status(405).end(`Method ${req.method} Not Allowed`);
  }

}

async function evaluateWithLLM(question, user_answer) {
  // Here you would call your local LLM
  // For demonstration, we'll simulate a response
  return {
    evaluation_result: user_answer === "correct" ? "Correct" : "Incorrect",
    correct_answer: "correct", // This should be dynamically generated by your LLM
  };
}
